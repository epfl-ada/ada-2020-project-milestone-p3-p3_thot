{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oraziorillo/Programs/anaconda3/envs/ada/lib/python3.8/site-packages/tqdm/std.py:697: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import nltk\n",
    "import re\n",
    "import pickle\n",
    "from discourseMarkers import rule\n",
    "from  nltk.parse.corenlp import CoreNLPParser\n",
    "from nltk import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from tqdm import tqdm\n",
    "from resources import lemmatizer, premise_conclusion_markers, english_dictionary, wordnet\n",
    "\n",
    "# Create and register a new `tqdm` instance with `pandas`\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = pd.read_csv(\"data/data.csv\").drop(columns=['URLs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invert_quotes_fullstop(sentence):\n",
    "    '''\n",
    "    Substitute the expression '.\"' with '\".' -> done to help sentence tokenization\n",
    "    '''\n",
    "    return sentence.replace('.”', '”.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sentences(article):\n",
    "    sentences = []\n",
    "    if type(article) == str:\n",
    "        for period in article.split(\"\\n\"):\n",
    "            for sentence in sent_tokenize(period):\n",
    "                sentences.append(sentence)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_future_verbs(sentence):        \n",
    "    text = word_tokenize(sentence)\n",
    "    tagged = pos_tag(text)\n",
    "    return len([word for word in tagged if word[1] in [\"VBC\", \"VBF\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_premise_conclusion_markers(sentence):        \n",
    "    counter = 0\n",
    "    for marker in premise_conclusion_markers:\n",
    "        if sentence.find(marker) != -1:\n",
    "            counter += 1\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    '''\n",
    "    Helper function to convert a POS tag from the Penn Treebank to a WordNet format\n",
    "    :param treebank_tag: Penn Treebank POS tag, string\n",
    "    :returns: WordNet POS tag\n",
    "    '''\n",
    "\n",
    "    if treebank_tag.startswith(\"J\"):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith(\"V\"):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith(\"N\"):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith(\"R\"):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_words(words):\n",
    "    '''\n",
    "    Function to lemmatize word tokens using the WordNet lemmatizer\n",
    "    :param words: list of word tokens in the tweet\n",
    "    :returns: list of lemmatized word tokens\n",
    "    '''\n",
    "\n",
    "    lemmatized_words = []\n",
    "    # Get the Penn Treebank POS tags for the word tokens\n",
    "    word_pos_tags = pos_tag(words)\n",
    "    for word, word_pos_tag in word_pos_tags:\n",
    "        # Get the WordNet POS tag\n",
    "        word_pos_tag = get_wordnet_pos(word_pos_tag)\n",
    "        # Use the WordNet POS tag to lemmatize the word into the correct word form\n",
    "        lemmatized_words.append(lemmatizer.lemmatize(word, word_pos_tag))\n",
    "    return lemmatized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_tokens(sentence):\n",
    "    '''\n",
    "    Function to retrieve only tokens corresponding to real English words\n",
    "    :param sentence: string corresponding to an English sentence\n",
    "    :returns: parsed list of tokens with only word tokens\n",
    "    '''\n",
    "    tokens = word_tokenize(sentence)\n",
    "    return [token for token in tokens if re.match(r\"^[A-Za-z\\']+$\", token)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tree(sentence):\n",
    "    ''' Create the parse tree for the given sentence '''\n",
    "    parser = CoreNLPParser()\n",
    "    return next(parser.raw_parse(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_linguisic_features_article(article):\n",
    "    \n",
    "    features = dict()\n",
    "    \n",
    "    if type(article) != str:\n",
    "        features['number_of_future_verbs'] = float('nan')\n",
    "        features['number_of_premise_conclusion_markers'] = float('nan')\n",
    "        features['number_of_sentences'] = float('nan')\n",
    "        features['average_sentence_lenght'] = float('nan')\n",
    "        features['number_of_words'] = float('nan')\n",
    "        features['average_word_lenght'] = float('nan')\n",
    "        features['dictionary_percentage'] = float('nan')\n",
    "        return features\n",
    "    \n",
    "    clean_article = invert_quotes_fullstop(article)\n",
    "    \n",
    "    sentences = split_sentences(clean_article)\n",
    "    words = get_word_tokens(clean_article)\n",
    "    \n",
    "    # Compute the number of verbs at future tense\n",
    "    features['number_of_future_verbs'] = count_future_verbs(clean_article)\n",
    "    \n",
    "    # Compute the number of premise and conclusion markers\n",
    "    features['number_of_premise_conclusion_markers'] = count_premise_conclusion_markers(clean_article)\n",
    "    \n",
    "    # Compute the total number of sentences\n",
    "    features['number_of_sentences'] = len(sentences)\n",
    "    \n",
    "    # Compute the average lenght of article's sentences\n",
    "    features['average_sentence_lenght'] = np.mean([len(s.split()) for s in sentences])\n",
    "    \n",
    "    # Compute the number of words in the article\n",
    "    num_words = len(words)\n",
    "    features['number_of_words'] = num_words\n",
    "    \n",
    "    # Compute the average lenght of the words in the article\n",
    "    features['average_word_lenght'] = np.mean([len(w) for w in words])\n",
    "    \n",
    "    # Compute the proportion of words belonging to the dictionary\n",
    "    dictionary_words = [word for word in lemmatize_words(words) if word in english_dictionary]\n",
    "    num_dictionary_words = len(dictionary_words)\n",
    "    dictionary_percentage = num_dictionary_words / num_words if num_words > 0 else 0\n",
    "    features['dictionary_percentage'] = dictionary_percentage\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4009/4009 [03:52<00:00, 17.24it/s]\n"
     ]
    }
   ],
   "source": [
    "# Extract features from each article and put them in a dataframe\n",
    "features_df = data.progress_apply(lambda x: pd.Series(extract_linguisic_features_article(x.Body)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number_of_future_verbs</th>\n",
       "      <th>number_of_premise_conclusion_markers</th>\n",
       "      <th>number_of_sentences</th>\n",
       "      <th>average_sentence_lenght</th>\n",
       "      <th>number_of_words</th>\n",
       "      <th>average_word_lenght</th>\n",
       "      <th>dictionary_percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>18.946429</td>\n",
       "      <td>1081.0</td>\n",
       "      <td>4.719704</td>\n",
       "      <td>0.823312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>26.285714</td>\n",
       "      <td>538.0</td>\n",
       "      <td>4.657993</td>\n",
       "      <td>0.797398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>20.111111</td>\n",
       "      <td>1070.0</td>\n",
       "      <td>4.824299</td>\n",
       "      <td>0.844860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>32.333333</td>\n",
       "      <td>90.0</td>\n",
       "      <td>5.033333</td>\n",
       "      <td>0.877778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>41.0</td>\n",
       "      <td>4.243902</td>\n",
       "      <td>0.658537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4004</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>13.583333</td>\n",
       "      <td>322.0</td>\n",
       "      <td>4.298137</td>\n",
       "      <td>0.826087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4005</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.142857</td>\n",
       "      <td>52.0</td>\n",
       "      <td>3.903846</td>\n",
       "      <td>0.711538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4006</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4007</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>24.307692</td>\n",
       "      <td>311.0</td>\n",
       "      <td>5.032154</td>\n",
       "      <td>0.893891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4008</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>19.600000</td>\n",
       "      <td>581.0</td>\n",
       "      <td>4.667814</td>\n",
       "      <td>0.779690</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4009 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      number_of_future_verbs  number_of_premise_conclusion_markers  \\\n",
       "0                        0.0                                   4.0   \n",
       "1                        0.0                                   3.0   \n",
       "2                        0.0                                   2.0   \n",
       "3                        0.0                                   3.0   \n",
       "4                        0.0                                   2.0   \n",
       "...                      ...                                   ...   \n",
       "4004                     0.0                                   3.0   \n",
       "4005                     0.0                                   2.0   \n",
       "4006                     NaN                                   NaN   \n",
       "4007                     0.0                                   2.0   \n",
       "4008                     0.0                                   3.0   \n",
       "\n",
       "      number_of_sentences  average_sentence_lenght  number_of_words  \\\n",
       "0                    56.0                18.946429           1081.0   \n",
       "1                    21.0                26.285714            538.0   \n",
       "2                    54.0                20.111111           1070.0   \n",
       "3                     3.0                32.333333             90.0   \n",
       "4                     1.0                37.000000             41.0   \n",
       "...                   ...                      ...              ...   \n",
       "4004                 24.0                13.583333            322.0   \n",
       "4005                  7.0                 8.142857             52.0   \n",
       "4006                  NaN                      NaN              NaN   \n",
       "4007                 13.0                24.307692            311.0   \n",
       "4008                 30.0                19.600000            581.0   \n",
       "\n",
       "      average_word_lenght  dictionary_percentage  \n",
       "0                4.719704               0.823312  \n",
       "1                4.657993               0.797398  \n",
       "2                4.824299               0.844860  \n",
       "3                5.033333               0.877778  \n",
       "4                4.243902               0.658537  \n",
       "...                   ...                    ...  \n",
       "4004             4.298137               0.826087  \n",
       "4005             3.903846               0.711538  \n",
       "4006                  NaN                    NaN  \n",
       "4007             5.032154               0.893891  \n",
       "4008             4.667814               0.779690  \n",
       "\n",
       "[4009 rows x 7 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.to_csv(\"data/features.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not run, this is the code to store the discourse markers\n",
    "\n",
    "discourse_markers = []\n",
    "\n",
    "for i in tqdm(range(len(data_raw))):\n",
    "    article_dms = []\n",
    "    article = data_raw.loc[i].Body\n",
    "    if type(article) == str:\n",
    "        article = invert_quotes_fullstop(article)\n",
    "        for s in split_sentences(article):\n",
    "            try:\n",
    "                clean_sentence = clean(s)\n",
    "                if len(clean_sentence) > 450 or len(clean_sentence)==0:\n",
    "                    continue\n",
    "                if i==3477:\n",
    "                    tree = parse_tree(clean_sentence[:330])\n",
    "                else:\n",
    "                    tree = parse_tree(clean_sentence)\n",
    "                sentence_dms = rule(tree.pformat())\n",
    "                if sentence_dms != 0:\n",
    "                    article_dms += sentence_dms\n",
    "            except IndexError:\n",
    "                continue\n",
    "    discourse_markers.append(article_dms)\n",
    "\n",
    "with open('discourseMarkers.data'.format(40), 'wb') as filehandle:\n",
    "    # store the data as binary data stream\n",
    "    pickle.dump(discourse_markers, filehandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the discourse markers list\n",
    "with open('discourseMarkers.data', 'rb') as filehandle:\n",
    "    # read the data as binary data stream\n",
    "    discourse_markers = pickle.load(filehandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4009"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(discourse_markers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
